\documentclass[11pt,a4paper]{article}
\usepackage{fullpage}
\usepackage[T1]{fontenc} 
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage[table,dvipsnames]{xcolor}
\usepackage[hidelinks]{hyperref}
\usepackage[polish]{babel}
\usepackage{menukeys}
\usepackage{subcaption}

\setlength{\parindent}{0cm}
\setlength{\parskip}{2mm}
\newcolumntype{Y}{>{\centering\arraybackslash}X}
\DeclareMathOperator{\sgn}{sgn}

\begin{document}

\title{Rozpoznawanie człowieka metodami biometrii \\
\Large{
    Projekt 3. --- Rozpoznawanie na~podstawie twarzy \\
    Raport
}}
\author{Bartłomiej Dach}
\maketitle
Poniższy dokument stanowi sprawozdanie z~implementacji aplikacji dokonującej rozpoznwania człowieka na~podstawie zdjęć twarzy w~pozycji frontalnej.
W~dokumencie opisano porównywane metody oraz~zawarto wyniki eksperymentalne dla~obu metod na~dostarczonym zbiorze zdjęć.

\section{Wstęp}

Twarz to~cecha biometryczna zaliczająca~się do~cech biologicznych.
Większość mierzalnych cech twarzy (geometria, kolor skóry) jest opartych na~cechach anatomicznych lub~etnicznych.
Z~drugiej strony aspekty takie, jak~zarost czy~wiek utrudniają wykorzystanie tych~cech, ponieważ zmieniają~się z~czasem.

Rozpoznawanie na~podstawie twarzy jest częścią coraz większej liczby urządzeń konsumenckich, szczególnie w~branży urządzeń mobilnych.
Ponieważ akwizycja obrazu twarzy nie~wymaga bezpośredniego kontaktu, a~coraz~więcej smartfonów posiada kamery po~przedniej stronie, jest to wygodna metoda potwierdzenia tożsamości użytkownika telefonu.

W~ramach projektu zaimplementowano dwie metody oparte na~identyfikacji punktów charakterystycznych twarzy.
W~obu przypadkach do~znajdowania twarzy na~zdjęciach i~lokalizacji punktów charakterystycznych wykorzystano bibliotekę \texttt{dlib}.
Metody różnią~się właściwym przetwarzaniem punktów --- w jednej metodzie punkty charakterystyczne wykorzystywane~są bezpośrednio po~pewnych korekcyjnych przekształceniach geometrycznych, zaś w~drugiej w~celu porównania z~rozwiązaniami \emph{state-of-the-art} wykorzystano wytrenowany klasyfikator ResNet \cite{he2015, king2015}.
Dokładniejsze informacje znajdują~się w~podrozdziale~\ref{subsec:classification}.

\section{Opis aplikacji}

Opracowana aplikacja została zaimplmentowana w~języku skryptowym Python w~wersji 3.5.2.
Wybór umotywowany był minimalizacją czasu implementacji poprzez wykorzystanie istniejących bibliotek \emph{open source}.
Aplikacja ma postać zbioru skryptów konsolowych --- zdecydowano~się na~rezygnację z~interfejsu graficznego ze~względu na~jego niewielką przydatność w~stosunku do~czasu implementacji.
Sposób wywołania skryptów opisano w~podrozdziale~\ref{sec:manual}.

\subsection{Zastosowane biblioteki}

W~implementacji zastosowano zbiór bibliotek \emph{open source} ułatwiających implementację systemu.
Pełna lista zastosowanych bibliotek znajduje~się w~tabeli~\ref{tbl:libraries}.

\begin{table}[H]
    \begin{tabularx}{\textwidth}{|r|l|X|l|c|}
        \hline
        Nr & Nazwa & Opis & Licencja & \\
        \hline
        \hline
        1 & \texttt{dlib} 19.17.0 & Biblioteka wspomagająca proces rozpoznawania twarzy & Boost & \cite{king2003} \\
        \hline
        2 & \texttt{matplotlib} 3.0.3 & Tworzenie wykresów i~wizualizacji & PSF & \cite{hunter2007} \\
        \hline
        3 & \texttt{pandas} 0.24.2 & Struktury do~manipulacji i~analizy danych & BSD & \cite{mckinney2010} \\
        \hline
        4 & \texttt{seaborn} 0.9.0 & Rozszerzone wizualizacje danych & BSD & \cite{waskom2018} \\
        \hline
        5 & \texttt{tqdm} 4.31.1 & Biblioteka wspomagająca do pasków postępu w~skryptach & MPL & \cite{dacosta2016} \\
        \hline
    \end{tabularx}
    \caption{Lista bibliotek użytych w~projekcie}
    \label{tbl:libraries}
\end{table}

\subsection{Instrukcja obsługi}
\label{sec:manual}

W~celu uruchomienia skryptów do~rozpoznawania konieczne jest zainstalowanie interpretera~Python oraz~bibliotek zawartych w~tabeli~\ref{tbl:libraries}.
Aby~zainstalować wymagane biblioteki, należy wywołać polecenie
\begin{verbatim}
$ pip3 install -r requirements.txt
\end{verbatim}
gdzie plik \texttt{requirements.txt} to~plik dołączony do~źródeł aplikacji

\subsubsection{Skrypt \texttt{recognize.py}}

Skrypt \texttt{recognize.py} pozwala na~sprawdzenie, czy twarze na~podanym zdjęciu należą do~osób z~podanego zbioru treningowego.
Wykonywanie skryptu powinno być zgłodne ze~składnią:

\begin{minipage}{\linewidth}
\begin{verbatim}
recognize.py [-h] (-d TRAINING_SET_DIR | -f EMBEDDING_FILE)
             [-l LANDMARK_MODEL] (-n | -r RECOGNITION_MODEL)
             [-t THRESHOLD] [-o OUTPUT_EMBEDDINGS]
             TEST_IMAGE
\end{verbatim}
\end{minipage}

gdzie poszczególne argumenty to:
\begin{itemize}
    \item \verb+-h+ --- wyświetla pomoc,
    \item \verb+-d TRAINING_SET_DIR+ --- oznacza, że wektory cech dla zdjęć ze~zbioru testowego powinny~być obliczone na~podstawie zdjęć z~podanego folderu \verb+TRAINING_SET_DIR+,
    \item \verb+-f EMBEDDING_FILE+ --- oznacza, że wektory cech dla zdjęć ze~zbioru testowego powinny~być wczytane z~istniejącego pliku \verb+EMBEDDING_FILE+,
    \item \verb+-l LANDMARK_MODEL+ --- zawiera ścieżkę do~pliku zawierającego model używany do~lokalizacji punktów charakterystycznych twarzy.
        W~przypadku braku tego parametru domyślnie zachodzi próba użycia pliku o~nazwie
        \begin{verbatim}shape_predictor_68_face_landmarks.dat\end{verbatim}
        z~katalogu roboczego.
    \item \verb+-n+ --- oznacza, że do~rozpoznawania ma~być zastosowany własny algorytm normalizacji punktów charakterystycznych,
    \item \verb+-r RECOGNITION_MODEL+ --- oznacza, że do~rozpoznawania ma~być zastosowany wytrenowany model ResNet załadowany z~pliku \verb+RECOGNITION_MODEL+,
    \item \verb+-t+ --- oznacza próg liczbowy, którego przekroczenie powoduje odrzucenie zdjęcia ze~zbioru testowego przy~klasyfikacji twarzy,
    \item \verb+-o OUTPUT_EMBEDDINGS+ --- pozwala na~zapisanie wektorów cech po~wyznaczeniu punktów charakterystycznych do~pliku,
    \item \verb+TEST_IMAGE+ --- ścieżka do~zdjęcia z~twarzami do~identyfikacji.
\end{itemize}

\subsubsection{Skrypt \texttt{confusion\_verification.py}}

Skrypt \texttt{confusion\_verification.py} dokonuje testu wybranego klasyfikatora dla~zadania weryfikacji tożsamości.
Składnia wywołania powinna być zgodna z~następującym wzorcem:

\begin{minipage}{\linewidth}
\begin{verbatim}
confusion_verification.py [-h]
                          (-d TRAINING_SET_DIR | -f EMBEDDING_FILE)
                          [-l LANDMARK_MODEL]
                          (-n | -r RECOGNITION_MODEL)
                          [-o OUT_FILE_PREFIX]
\end{verbatim}
\end{minipage}

Wszystkie parametry poza \verb+-o+ mają to~samo znaczenie, co w~przypadku skryptu \texttt{recognize.py}.
Flaga \verb+-o+ oznacza w~tym przypadku prefiks dla~plików tworzonych przez skrypt.

Wynikiem skryptu są~dwa pliki:
\begin{enumerate}
    \item Pierwszy plik o~sufiksie \verb+distance_matrix.csv+ zawiera symetryczną macierz odległości Euklidesa między wektorami cech wyznaczonymi przez~klasyfikator na~podstawie punktów charakterystycznych.
    \item Drugi plik o~sufiksie \verb+error_rates.csv+ zawiera stosunki błędów w~zadaniu weryfikacji w~zależności od~przyjętego progu.
        Dla każdej pary obrazów klasyfikator uznaje dwa obrazy za~obrazy tej samej osoby, jeśli odległość Euklidesa między ich~wektorami cech jest mniejsza niż~przyjęty próg.
        Na~tej podstawie obliczane~są wskaźniki FAR i~FRR.
\end{enumerate}

\subsubsection{Skrypt \texttt{confusion\_identification.py}}

Skrypt \texttt{confusion\_identification.py} dokonuje testu wybranego klasyfikatora dla zadania identyfikacji osoby na~zdjęciu.
Składnia wywołania powinna być zgodna z~następującym wzorcem:

\begin{minipage}{\linewidth}
\begin{verbatim}
confusion_identification.py [-h]
                            (-d TRAINING_SET_DIR | -f EMBEDDING_FILE)
                            [-l LANDMARK_MODEL]
                            (-n | -r RECOGNITION_MODEL)
                            [-o OUT_FILE_PREFIX]
\end{verbatim}
\end{minipage}

gdzie parametry są~interpretowane tak samo, jak w~skrypcie \verb+confusion_identification.py+.
Wynikiem skryptu jest pojedynczy plik o~sufiksie \verb+confusion_matrix.csv+, zawierający macierz pomyłek obliczoną dla~klasyfikatora.
Wiersze macierzy oznaczają etykietę osoby na~zdjęciu (wzorcową), zaś kolumny --- etykietę zwróconą przez~klasyfikator.

\section{Opis metody}

Proces rozpoznawania na~podstawie zdjęcia można podzielić na~dwa podstawowe etapy.
Pierwszy z~nich zajmuje~się lokalizacją twarzy na~obrazie, poprzez określenie prostokątnego obszaru, w~którym zawierają~się znalezione twarze.
Drugi zaś zajmuje~się faktycznym rozpoznawaniem.

Istnieje wiele podejść do~rozpoznawania; część z~nich, jak np.~metoda twarzy własnych, operuje bezpośrednio na~znalezionym fragmencie obrazu z~twarzą, podczas gdy~inne opierają~się na~lokalizacji i~przetwarzaniu tzw.~punktów charakterystycznych, czyli ustalonych punktów związanych z~anatomią i~geometrią twarzy.
W~ramach projektu skorzystano z~tego drugiego podejścia.

Po~wyznaczeniu punktów charakterystycznych następuje proces ich przekształcenia na~cechy, które wykorzystywane są przez~końcowy klasyfikator.
W~projekcie porównano dwa podejścia, opisane szerzej w~podrozdziale~\ref{subsec:classification}.

\subsection{Rozpoznawanie twarzy na~zdjęciu}

Do~implementacji etapu rozpoznawania twarzy wykorzystane zostały gotowe elementy biblioteki \texttt{dlib}.
Wbudowany w~bibliotekę detektor twarzy w~postaci frontalnej wykorzystuje metodę histogramu zorientowanych gradientów (ang. \emph{Histogram of~Oriented Gradients}, HOG) \cite{dalal2005}.

Ta~metoda po~wstępnej normalizacji i~korekcji gamma oblicza gradient obrazu poprzez~zastosowanie filtrów splotowych do~wykrywania krawędzi.
Następnie gradienty w~poszczególnych punktach są~gromadzone w~tzw.~komórki, w~ramach których następuje ważone uśredninanie.
Później, aby zminimalizować wpływ oświetlenia, wykonywana jest lokalna normalizacja kontrastu.
Na~koniec dwuwymiarowe bloki są~okienkowane (wycinane są~fragmenty obrazu), i~wynikowe okna są~podawane do~klasyfikatora, decydującego, czy układ gradientów w~ramach okna jest zgodny z~szukanym typem obiektu --- w~tym przypadku, frontalnym zdjęciem twarzy.

\subsection{Lokalizacja punktów charakterystycznych}

Po~zlokalizowaniu prostokata ograniczającego twarz, następuje lokalizacja punktów charakterystycznych.
Istnieje wiele propozycji zbiorów punktów wykorzystywanych przy~identyfikacji; w~tym przypadku zdecydowano~się zastosować układ 68~punktów przedstawionych na~rysunku~\ref{fig:68-landmarks}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{res/img/figure_68_markup.jpg}
    \caption{Wizualizacja 68~punktów charakterystycznych wykorzystywanych do~rozpoznawania twarzy, wytypowanych przez~\emph{Intelligent Behaviour Understanding Group}. Źródło: \cite{sagonas2013}}
    \label{fig:68-landmarks}
\end{figure}

W~projekcie zastosowano gotowy model predykcyjny wyznaczający punkty charakterystyczne, wytrenowany na~ręcznie oetykietowanym zbiorze danych ibug~300-W~\cite{king2015}.
Lokalizuje on~punkty charakterystyczne dla~znalezionych twarzy, które następnie wykorzystywane~są do~ekstrakcji cech.

\subsection{Klasyfikacja twarzy na~podstawie punktów charakterystycznych}
\label{subsec:classification}

Przy~klasyfikacji zastosowano dwa podejścia.
Pierwsze, nazwane odtąd normalizacyjnym, wykorzystuje wyliczone punkty charakterystyczne, próbując wyeliminować wpływ obrotów i~skalowania na~działanie klasyfikatora.
Z~kolei drugie opiera~się na~sieciach neuronowych opartych na~architekturze ResNet~\cite{he2015}.

\subsubsection{Podejście normalizacyjne}

Przy~podejściu normalizacyjnym celem jest wyeliminowanie rotacji i~skalowania.
W~celu wyeliminowania rotacji założono referencyjnie, że~linia od~lewego kącika lewego oka (punkt nr~37) do~prawego kącika prawego oka (punkt nr~46) powinna być pozioma, zaś~grzbiet nosa (linia od~punktu~28 do~punktu~34) --- pionowa.

Aby to~osiągnąć, zastosowano dwie~operacje ścinania wzdłuż~obu osi obrazu, przeprowadzające $i$-ty punkt charakterystyczny~$(x_i, y_i)$ ($i = 1,\dots,68$) na~punkt~$(x'_i, y'_i)$, którego współrzędne są~określone wzorami
\begin{align*}
    x_i' &= x_i - y_i \cdot \frac{x_{34} - x_{27}}{y_{34} - y_{27}} \\
    y_i' &= y_i - x_i \cdot \frac{y_{46} - y_{37}}{x_{46} - y_{37}}
\end{align*}
Następnie w~celu wyeliminowania wpływu skalowania punkty charakterystyczne zostały przeprowadzone na~kwadrat $[-0.5, 0.5] \times [-0.5, 0.5]$, zgodnie z~wzorami
\begin{align*}
    x_i'' &= \frac{x'_i - x'_{\min}}{x'_{\max} - x'_{\min}} - 0.5 \\
    y_i'' &= -\frac{y'_i - y'_{\min}}{y'_{\max} - y'_{\min}} + 0.5
\end{align*}
Zmiana znaku na~współrzędnej~Y uzasadniona jest tym, że~piksele obrazów są~indeksowane odwrotnie niż~przyjmuje konwencja matematyczna (tj. współrzędna~Y piksela rośnie w~dół obrazu).
Po~tych dwóch operacji współrzędne X~i~Y obrazu są~traktowane jako cechy do~klasyfikacji --- to~podejście skutkuje więc uzyskaniem wektora złożonego z~$2 \cdot 68 = 136$ cech.
Warto zauważyć, że podejście to~ignoruje barwy obrazu, lecz korzysta wyłącznie z~geometrycznych cech twarzy.

\subsubsection{Wykorzystanie klasyfikatora ResNet}

Dla~porównania jako~drugą metodę wykorzystano wytrenowany wcześniej klasyfikator dostarczony przez~twórców biblioteki \texttt{dlib}.
Tym klasyfikatorem jest sieć neuronowa o~zmodyfikowanym modelu ResNet~\cite{he2015}, o~29 warstwach konwolucyjnych, wytrenowana na~zbiorze danych składającym~się z~ok. 3~milionów twarzy.
Klasyfikator transformuje punkty charakterystyczne na~wektor w~128-wymiarowej przestrzeni.
Funkcja błędu była skonstruowana tak, aby~punkty korespondujące z~zdjęciami twarzy jednej osoby stanowiły rozłączne kule o~promieniu 0.6 \cite{king2015}.

\subsubsection{Finalna klasyfikacja}

W~przypadku obu klasyfikatorów przyjęto najprostszą klasyfikację wyjściowych wektorów --- klasyfikator 1-NN.
Klasyfikacja polega więc na~wyborze wektora reprezentującego twarz ze~zbioru referencyjnego o~najmniejszej odległości w~metryce Euklidesowej od~wektora wyznaczonego dla~identyfikowanej twarzy.

\section{Wyniki eksperymentalne}
\label{sec:results}

\begin{figure}[H]
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.5\textwidth]{res/img/normalized_distance_matrix.png}
        \label{subfig:normalized-distance-matrix}
        \caption{Macierz dla~podejścia normalizacyjnego po~przetworzeniu odległości wg~wzoru~$d' = 1 - \frac{1}{d + 1}$.}
    \end{subfigure}
    \\
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.5\textwidth]{res/img/resnet_distance_matrix.png}
        \label{subfig:resnet-distance-matrix}
        \caption{Macierz dla~klasyfikatora ResNet.}
    \end{subfigure}
    \caption{Macierze wzajemnych odległości dla~poszczególnych par obrazów ze~zbioru testowego.
    Zauważalne są ciemniejsze bloki wzdłuż~przekątnej, obrazujące podobieństwo wielu obrazów przedstawiających jedną osobę.}
\end{figure}

\begin{figure}[H]
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{res/img/normalized_error_rates.pdf}
        \label{subfig:normalized-error-rates}
        \caption{Wskaźniki FAR i~FRR dla~podejścia normalizacyjnego.}
    \end{subfigure}
    \\
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{res/img/resnet_error_rates.pdf}
        \label{subfig:resnet-error-rates}
        \caption{Wskaźniki FAR i~FRR dla~klasyfikatora ResNet.}
    \end{subfigure}
    \caption{Wartości wskaźników FAR i~FRR w~zależności od~przyjętego progu klasyfikacji w~zadaniu weryfikacji tożsamości na~podstawie par zdjęć.}
\end{figure}

\begin{figure}[H]
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{res/img/normalized_confusion_matrix.pdf}
        \label{subfig:normalized-distance-matrix}
        \caption{Macierz dla~podejścia normalizacyjnego.}
    \end{subfigure}
    \\
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{res/img/resnet_confusion_matrix.pdf}
        \label{subfig:resnet-distance-matrix}
        \caption{Macierz dla~klasyfikatora ResNet.}
    \end{subfigure}
    \caption{Macierze pomyłek dla~obu podejść.
    Wiersze odpowiadają etykietom (identyfikatorom osób) oryginalnych zdjęć,
    kolumny odpowiadają etykietym przypisanym zdjęciom przez klasyfikator.}
\end{figure}

\subsection{Podejście normalizacyjne}

\subsection{Klasyfikator ResNet}

\section{Podsumowanie}

\begin{thebibliography}{9}

    \bibitem{dacosta2016}
        da~Costa-Luis, C. i~inni,
        ,,tqdm''.
        [Online]
        \\
        Dostępne: \url{https://tqdm.github.io/}.
        [Dostęp 11~maja~2019]

    \bibitem{dalal2005}
        Dalal, N.,
        Triggs, B.,
        ,,Histograms of~Oriented Gradients for~Human Detection'',
        \emph{2005 IEEE Computer Society Conference on~Computer Vision and~Pattern Recognition}.

    \bibitem{he2015}
        He, K.,
        Zhang, X.,
        Ren, S.,
        Sun, J.,
        ,,Deep Residual Learning for Image Recognition'',
        {\tt arXiv:1512.03385},
        2015.
        [Online]
        \\
        Dostępne: \url{https://arxiv.org/abs/1512.03385}.
        [Dostęp 11~maja~2019]

    \bibitem{hunter2007}
        ,,Matplotlib: A~2D~graphics environment'',
        \emph{Computing In~Science \& Engineering},
        tom~9,
        nr~3,
        s.~90--95,
        2007.

    \bibitem{king2003}
        King, D. i~inni,
        ,,dlib C++ Library''.
        [Online]
        \\
        Dostępne: \url{http://dlib.net/}.
        [Dostęp 11~maja~2019]

    \bibitem{king2015}
        King, D.,
        ,,dlib-models''.
        [Online]
        \\
        Dostępne: \url{https://github.com/davisking/dlib-models}.
        [Dostęp 11~maja~2019]

    \bibitem{mckinney2010}
        McKinney, W.,
        ,,Data Structures for~Statistical Computing in~Python'',
        \emph{Proceedings of~the~9\textsuperscript{th} Python in~Science Conference},
        s.~51--56,
        2010.

    \bibitem{oliphant2006}
        Oliphant, T.E.,
        \emph{A Guide to NumPy},
        Trelgol~Publishing,
        Stany Zjednoczone,
        2006.

    \bibitem{sagonas2013}
        Sagonas, C.,
        Zafeiriou, S.,
        ,,Facial point annotations''.
        [Online]
        \\
        Dostępne: \url{https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/}.
        [Dostęp 11~maja~2019]

    \bibitem{waskom2018}
        Waskom, M. i~inni,
        ,,\texttt{seaborn}: statistical data visualization''.
        [Online]
        \\
        Dostępne: \url{https://seaborn.pydata.org/}.
        [Dostęp 11~maja~2019]

\end{thebibliography}

\end{document}
